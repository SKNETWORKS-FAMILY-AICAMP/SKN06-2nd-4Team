{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "## 한글 처리\n",
    "plt.rcParams['font.family'] = 'malgun gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/credit_card_churn.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['churn'].value_counts().plot(kind='bar', color=['skyblue', 'lightcoral'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 칼럼명 수정 + 소문자화\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_columns = {\n",
    "        'Attrition_Flag': 'churn',\n",
    "        'Customer_Age' : 'age',\n",
    "        'Dependent_count' : 'dependent_cnt',\n",
    "        'Months_on_book' : 'card_usage_period',\n",
    "        'Total_Relationship_Count' : 'account_cnt',\n",
    "        'Months_Inactive_12_mon' : 'inactive_month_in_year',\n",
    "        'Contacts_Count_12_mon' : 'visit_cnt_in_year',\n",
    "        'Total_Revolving_Bal' : 'revolving_balance',\n",
    "        'Avg_Open_To_Buy' : 'avg_remain_credit_limit',\n",
    "        'Total_Amt_Chng_Q4_Q1' : 'total_amt_change_q4_q1',\n",
    "        'Total_Trans_Ct' : 'total_trans_cnt',\n",
    "        'Total_Ct_Chng_Q4_Q1' : 'total_cnt_change_q4_q1'\n",
    "    }\n",
    "data.rename(columns=rename_columns, inplace=True)\n",
    "data.columns = data.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불필요 칼럼 삭제\n",
    "\n",
    "- clientnum : 회원번호\n",
    "- naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_1\n",
    "- naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\n",
    "    columns=[\n",
    "        'clientnum',\n",
    "        'naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_1',\n",
    "        'naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_2'\n",
    "    ], \n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결과값 'churn' mapping\n",
    "\n",
    "- Existing Customer: 0 \n",
    "- Attrited Customer: 1 (이탈)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['churn'] = data['churn'].map({\"Existing Customer\": 0, \"Attrited Customer\": 1})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결측치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 확인\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측치 처리 방법\n",
    "\n",
    "- income_category: 비율에 따른 대치\n",
    "- education_level, marital_status: 최빈값으로 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['education_level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['marital_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이상치\n",
    "\n",
    "### 이상치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR 기반으로 Outlier 값 조회 메소드\n",
    "\n",
    "def is_outlier(data, whis=1.5):\n",
    "    \"\"\"\n",
    "    IQR 기반으로 Outlier 값 조회 메소드\n",
    "    parameter\n",
    "        data: outlier를 찾을 데이터\n",
    "        whis: IQR에 몇배를 극단치 계산에 사용할 지 비율. rate를 크게하면 outlier범위를 넓게 잡는다. 작게 주면 범위를 좁게 잡는다.\n",
    "    return\n",
    "        bool type ndarray: 각 원소별 outlier 여부 (True: Outlier(이상치), False: 정상범위값)\n",
    "    \"\"\"\n",
    "    q1 = np.quantile(data, q=0.25)\n",
    "    q3 = np.quantile(data, q=0.75)\n",
    "    IQR = q3 - q1\n",
    "    return (data < q1 - IQR * whis) | (data > q3 + IQR * whis)\n",
    "\n",
    "\n",
    "def get_normal_range(data, whis=1.5):\n",
    "    \"\"\"\n",
    "    IQR 기반으로 정상범위 조회 메소드\n",
    "    parameter1\n",
    "        data: 조회할 대상 데이터\n",
    "        whis: IQR에 몇배를 극단치 계산에 사용할 지 비율. rate를 크게하면 outlier범위를 넓게 잡는다. 작게 주면 범위를 좁게 잡는다.\n",
    "    return\n",
    "        tuple: (lower_bound, upper_bound) - 정상범위의 하한값과 상한값\n",
    "    \"\"\"\n",
    "    q1 = np.nanquantile(data, q=0.25)\n",
    "    q3 = np.nanquantile(data, q=0.75)\n",
    "    IQR = q3 - q1\n",
    "    lower_bound = q1 - IQR * whis\n",
    "    upper_bound = q3 + IQR * whis\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 컬럼별 이상치 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### age\n",
    "- 나이\n",
    "- 정상범위의 최대 값으로 대체한다.\n",
    "  - 정상 범위를 넘어간 값들의 개수가 많지 않으므로 같은 값으로 변경해서 하나의 값으로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_trans_cnt'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_trans_cnt'].plot(kind='hist', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high = get_normal_range(data['total_trans_cnt'], whis=1.5)\n",
    "print(low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.query('total_trans_cnt > @high').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### total_trans_cnt\n",
    "- 총 거래 횟수\n",
    "- 정상범위의 최대 값으로 대체한다.\n",
    "  - 정상 범위를 넘어간 값들의 개수가 많지 않으므로 같은 값으로 변경해서 하나의 값으로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(data['total_trans_cnt'].describe(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total_trans_cnt'].plot(kind='hist', bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high = get_normal_range(data['total_trans_cnt'], whis=1.5)\n",
    "print(low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.query('total_trans_cnt > @high').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 전처리 정리\n",
    "- 결측치 처리\n",
    "  - income_category: 비율에 따른 대치\n",
    "  - education_level, marital_status: 최빈값으로 대체\n",
    "\n",
    "- 이상치 처리\n",
    "  - age, total_trans_ct\n",
    "    - 정상범위의 최대 값으로 대체한다.  \n",
    "- encoding\n",
    "  - gender \n",
    "    - 라벨 인코딩(Label Encoding)\n",
    "  - education_level \n",
    "    - 순서 인코딩 (Ordinal Encoding)\n",
    "  - marital_status, card_category \n",
    "    - 원핫 인코딩(One-Hot encoding) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoad 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataloader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataloader.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_dataset():\n",
    "    # 데이터 load\n",
    "    data = pd.read_csv(\"data/credit_card_churn.csv\", na_values=\"Unknown\")\n",
    "\n",
    "    # 컬럼명 변경\n",
    "    rename_columns = {\n",
    "        \"Attrition_Flag\": \"churn\",\n",
    "        \"Customer_Age\": \"age\",\n",
    "        \"Dependent_count\": \"dependent_cnt\",\n",
    "        \"Months_on_book\": \"card_usage_period\",\n",
    "        \"Total_Relationship_Count\": \"account_cnt\",\n",
    "        \"Months_Inactive_12_mon\": \"inactive_month_in_year\",\n",
    "        \"Contacts_Count_12_mon\": \"visit_cnt_in_year\",\n",
    "        \"Total_Revolving_Bal\": \"revolving_balance\",\n",
    "        \"Avg_Open_To_Buy\": \"avg_remain_credit_limit\",\n",
    "        \"Total_Amt_Chng_Q4_Q1\": \"total_amt_change_q4_q1\",\n",
    "        \"Total_Trans_Ct\": \"total_trans_cnt\",\n",
    "        \"Total_Ct_Chng_Q4_Q1\": \"total_cnt_change_q4_q1\",\n",
    "    }\n",
    "    data.rename(columns=rename_columns, inplace=True)\n",
    "    # 컬럼명 소문자로 변경\n",
    "    data.columns = data.columns.str.lower()\n",
    "\n",
    "    ## 컬럼 삭제\n",
    "    data.drop(\n",
    "        columns=[\n",
    "            \"clientnum\",\n",
    "            \"naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_1\",\n",
    "            \"naive_bayes_classifier_attrition_flag_card_category_contacts_count_12_mon_dependent_count_education_level_months_inactive_12_mon_2\",\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    X = data.drop(columns=\"churn\")\n",
    "    y = data[\"churn\"]\n",
    "    y = data['churn'].map({\"Existing Customer\": 0, \"Attrited Customer\": 1})\n",
    "    \n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 파이프라인 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## 사용자 정의 전처리기 생성\n",
    "\n",
    ">    - fit() 은 입력받은 데이터 X 와 y 를 사용해 변환할 때 사용할 값을 찾아 self 에 attribute로 저장한다.\n",
    ">    - transform() 은 fit() 에서 찾은 값으로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# 이상치 처리\n",
    "# age, total_trans_coun 전처리에 적용할 transformer 클래스\n",
    "## - 정상범위 최대값, 최소값으로 대체\n",
    "\n",
    "class OutlierTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, whis=1.5):\n",
    "        self.whis = whis\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        q1 = np.nanquantile(X, q=0.25)\n",
    "        q3 = np.nanquantile(X, q=0.75)\n",
    "        IQR = q3 - q1\n",
    "        self.lower_bound = q1 - IQR * self.whis\n",
    "        self.upper_bound = q3 + IQR * self.whis\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = np.where(X < self.lower_bound, self.lower_bound, X)\n",
    "        X_transformed = np.where(X_transformed > self.upper_bound, self.upper_bound, X_transformed)\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "# 결측치 처리\n",
    "class ProportionalImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.proportions = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # 각 열의 비율을 계산하여 저장\n",
    "        for column in X.columns:\n",
    "            counts = X[column].value_counts(normalize=True, dropna=True)\n",
    "            self.proportions[column] = counts\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for column, probs in self.proportions.items():\n",
    "            # 결측치 위치 찾기\n",
    "            missing_mask = X[column].isna()\n",
    "            if missing_mask.any():\n",
    "                # 비율에 따라 랜덤하게 값 채우기\n",
    "                X.loc[missing_mask, column] = np.random.choice(\n",
    "                    probs.index, size=missing_mask.sum(), p=probs.values\n",
    "                )\n",
    "        return X \n",
    "    \n",
    "class LabelEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoder = LabelEncoder()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoder.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.encoder.transform(X).reshape(-1, 1)  # 1D 배열을 2D로 변환하여 반환\n",
    "\n",
    "\n",
    "class OrdinalEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categories=[]):\n",
    "        print(categories)\n",
    "        self.encoder = OrdinalEncoder(categories=categories)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.encoder.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.encoder.transform(X)  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = load_dataset()\n",
    "# X = data[0]\n",
    "# y = data[1]\n",
    "# X.columns\n",
    "# for index, column in enumerate(X.columns):\n",
    "#     print(f\"Index: {index}, Column Name: {column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파이프라인구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from preprocessing import  OutlierTransformer, ProportionalImputer, LabelEncoderTransformer, OrdinalEncoderTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "\n",
    "\n",
    "# Pipeline을 이용한 전처리\n",
    "nullvalue_transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('income_imputer', ProportionalImputer(), [5]), # income_category\n",
    "        ('education_imputer', SimpleImputer(strategy='most_frequent'), [3, 4]), # education_level, marital_status\n",
    "        # ('marital_imputer', SimpleImputer(strategy='most_frequent'), [4]) # marital_status\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "outlier_transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('age_outlier', OutlierTransformer(), [3]), # age\n",
    "        ('total_trans_outlier', OutlierTransformer(), [16]) # total_trans_cnt\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "education_categories = [[\"Uneducated\", \"High School\", \"College\", \"Graduate\", \"Post-Graduate\", \"Doctorate\"]]\n",
    "income_categories = [['Less than $40K', '$120K +', '$40K - $60K', '$60K - $80K', '$80K - $120K']]\n",
    "\n",
    "encoder = ColumnTransformer(\n",
    "    [\n",
    "        ('gender_encoder', LabelEncoderTransformer(), [5]), # gender\n",
    "        ('education_encoder', OrdinalEncoder(categories=education_categories), [3]), # education_level\n",
    "        ('income_encoder', OrdinalEncoder(categories=income_categories), [2]), # income_category\n",
    "        ('marital_encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), [4]), # marital_status\n",
    "        ('card_encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), [7]) # card_category\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "preprocessor_pipeline = Pipeline([\n",
    "    (\"imputer\", nullvalue_transformer),\n",
    "    (\"outlier\", outlier_transformer),\n",
    "    (\"encoding\", encoder),\n",
    "], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8101, 19) (2026, 19) (8101,) (2026,)\n"
     ]
    }
   ],
   "source": [
    "from dataloader import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# X, y 분리\n",
    "X, y = load_dataset()\n",
    "\n",
    "# Train/Test/Validation set 나누기.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape,y_train.shape, y_test.shape,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83934038 0.16065962]\n",
      "[0.83934156 0.16065844]\n",
      "[0.83958539 0.16041461]\n",
      "[0.83909181 0.16090819]\n"
     ]
    }
   ],
   "source": [
    "# 비율 확인\n",
    "print(np.unique(y, return_counts=True)[1]/y.size)\n",
    "print(np.unique(y_train, return_counts=True)[1]/y_train.size)\n",
    "print(np.unique(y_test, return_counts=True)[1]/y_test.size)\n",
    "print(np.unique(y_valid, return_counts=True)[1]/y_valid.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8101, 24)\n"
     ]
    }
   ],
   "source": [
    "# X_train_preprocessed = preprocessor_pipeline.transform(X_train)\n",
    "# X_test_preprocessed = preprocessor_pipeline.transform(X_test)\n",
    "\n",
    "print(X_train_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 파이프라인 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/preprocessor.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "os.makedirs('models', exist_ok=True)\n",
    "joblib.dump(\n",
    "    preprocessor_pipeline,     # 저장할 모델/전처리기\n",
    "    \"models/preprocessor.pkl\"  # 저장경로. pickle로 저장된다.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 모델링\n",
    "\n",
    "## Baseline 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models\\\\best_xgb.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'auc': make_scorer(roc_auc_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "tree.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_train_pred_tree = tree.predict(X_train_preprocessed)\n",
    "y_train_proba_tree= tree.predict_proba(X_train_preprocessed)[:, 1]\n",
    "\n",
    "params = {\n",
    "    'criterion': ['gini', 'entropy'],  # 노드 분할 기준\n",
    "    'max_depth': [None, 10, 20, 30],   # 각 결정 트리의 최대 깊이를 설정\n",
    "    'min_samples_split': [2, 10, 20],  # 노드를 분할하기 위한 최소 샘플 수\n",
    "    'min_samples_leaf': [1, 5, 10],    # 리프 노드의 최소 샘플 수\n",
    "    'max_features': [None, 'sqrt', 'log2']  # 각 트리가 학습할 때마다 사용할 특성(feature)의 수\n",
    "}\n",
    "\n",
    "gs_tree = GridSearchCV(\n",
    "    estimator=tree,          \n",
    "    param_grid=params,  \n",
    "    scoring=scoring,\n",
    "    refit='accuracy',\n",
    "    cv=5,             \n",
    "    n_jobs=-1,         \n",
    ")\n",
    "\n",
    "gs_tree.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "best_param_tree = gs_tree.best_params_\n",
    "best_model_tree = gs_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Playdata\\miniconda3\\envs\\ml\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "# 4-2-2. Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. 학습 및 예측\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "y_train_pred_rf = rf.predict(X_train_preprocessed)\n",
    "y_train_proba_rf= rf.predict_proba(X_train_preprocessed)[:, 1]\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [100, 200, 300],    # 결정 트리(Decision Tree)의 개수\n",
    "    'max_depth': [5, 10, 15],           # 각 결정 트리의 최대 깊이를 설정\n",
    "    'max_features': ['sqrt', 'log2']    # 각 트리가 학습할 때마다 사용할 특성(feature)의 수\n",
    "}\n",
    "gs_rf = GridSearchCV(\n",
    "    estimator=rf,       \n",
    "    param_grid=params,     \n",
    "    scoring=scoring,\n",
    "    refit='accuracy',\n",
    "    cv=5,                      \n",
    "    n_jobs=-1,             \n",
    ")\n",
    "\n",
    "gs_rf.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# 5. Best Model: 최적의 하이파라미터로 만든 모델\n",
    "best_param_rf = gs_rf.best_params_\n",
    "best_model_rf = gs_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "\"\"\"\n",
    "====================================================================================\n",
    "주요 파라미터\n",
    "=======================================================a=============================\n",
    "- n_estimators: 부스팅 단계의 수 = 모델이 생성할 트리 개수\n",
    "- learning_rate: 학습률\n",
    "- max_depth: 각 결정 트리의 최대 깊이를 설정\n",
    "- subsample: 각 트리 학습에 사용되는 샘플의 비율\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "\n",
    "gb.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# 2. 모델 평가\n",
    "# Train set + Test set 평가\n",
    "y_train_pred_gb = gb.predict(X_train_preprocessed)\n",
    "y_train_proba_gb= gb.predict_proba(X_train_preprocessed)[:, 1]\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": [100, 200, 300],  #  부스팅 단계의 수 = 모델이 생성할 트리 개수\n",
    "    \"learning_rate\": [0.1],  # 학습률\n",
    "    \"max_depth\": [1, 2, 3, 4, 5],  # 각 결정 트리의 최대 깊이를 설정\n",
    "    \"subsample\": [0.5, 0.7],  # 샘플링 비율\n",
    "}\n",
    "\n",
    "gs_gb = GridSearchCV(\n",
    "    estimator=gb,           \n",
    "    param_grid=params,   \n",
    "    scoring=scoring,\n",
    "    refit='accuracy',\n",
    "    cv=5,                  \n",
    "    n_jobs=-1,            \n",
    ")\n",
    "\n",
    "gs_gb.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "best_param_gb = gs_gb.best_params_\n",
    "best_model_gb = gs_gb.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "  \n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# 2. 모델 평가\n",
    "# Train set + Test set 평가\n",
    "y_train_pred_xgb = xgb.predict(X_train_preprocessed)\n",
    "y_train_proba_xgb= xgb.predict_proba(X_train_preprocessed)[:, 1]\n",
    "\n",
    "fi = xgb.feature_importances_\n",
    "\n",
    "\n",
    "# 4. 최적의 매개변수 구하기 - GridSearchCV\n",
    "params = {\n",
    "    \"max_depth\":[1, 2, 3, 4, 5],            # 각 결정 트리의 최대 깊이를 설정\n",
    "    'learning_rate': [0.1],                 # 학습률\n",
    "    'n_estimators': [100, 200, 300],        # 부스팅 단계의 수 = 모델이 생성할 트리 개수\n",
    "    'subsample': [0.5, 0.7],                # 각 트리의 훈련에 사용되는 샘플 비율\n",
    "    'colsample_bytree': [0.5, 0.7, 1.0],    # 각 트리의 훈련에 사용되는 피처 비율\n",
    "    'gamma': [0, 0.1],                      # 노드 분할에 대한 최소 손실 감소\n",
    "    'reg_alpha': [0],                       # L1 정규화\n",
    "    'reg_lambda': [0.1]                     # L2 정규화\n",
    "}\n",
    "gs_xgb = GridSearchCV(\n",
    "    estimator=xgb,           \n",
    "    param_grid=params,   \n",
    "    scoring=scoring,\n",
    "    refit='accuracy',\n",
    "    cv=5,                  \n",
    "    n_jobs=-1,            \n",
    ")\n",
    "\n",
    "gs_xgb.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# 5. 튜닝 : Best Model 찾기\n",
    "best_param_xgb = gs_xgb.best_params_\n",
    "best_model_xgb = gs_xgb.best_estimator_\n",
    "\n",
    "best_y_pred_xgb = best_model_xgb.predict(X_test_preprocessed)\n",
    "best_y_proba_xgb= best_model_xgb.predict_proba(X_test_preprocessed)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 저장, 최종 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'models/'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "joblib.dump(best_model_tree, os.path.join(directory, 'best_tree.pkl'))\n",
    "joblib.dump(best_model_rf, os.path.join(directory, 'best_rf.pkl'))\n",
    "joblib.dump(best_model_gb, os.path.join(directory, 'best_gb.pkl'))\n",
    "joblib.dump(best_model_xgb, os.path.join(directory, 'best_xgb.pkl'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
